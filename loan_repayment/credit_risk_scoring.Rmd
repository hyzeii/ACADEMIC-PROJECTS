---
title: "R Notebook"
output:
  word_document: default
  html_document:
    df_print: paged
---

# I. Introduction

# II. Theorectical background

# III. Data

## Preparing data set

The data in this article comes from the Kaggle dataset, as read in the description of the dataset: "contains columns simulating credit bureau data." This means that the dataset is a simulation of what credit bureau data may have generated. If anyone here is planning to use this dataset for formal reasons, do note that it is not credible. A significant problem for credit scoring models which must be pointed out is the unavailability of real-world credit data. The reason is that customer's credit data is confidential in most of the financial institutions. Therefore, this paper will be for educational purpose only due to the lack of availability in real life.

Setting up working directory:

```{r}
setwd(getwd())
```

Load some packages:

```{r}
library(tidyverse)
library(dplyr)
library(readr)
library(data.table)
library(questionr)
library(corrplot)
library(superml)
```

```{r}
library(smotefamily)
library(rsample)
library(caret)
library(performanceEstimation)
library(Information)
library(ROSE)
library(scorecard)
library(ggcorrplot)
library(randomForest)
library(xgboost)

```

Import data set:

```{r}
# setwd("D:\Uni\R\FRM")
df <- read_csv('credit_risk_dataset.csv')
head(df)
```

Basic descriptive statistic

```{r}
dim(df)
str(df)
glimpse(df)
summary(df)
```

Check for imbalance of the dependent variable

```{r}
# Data Overview
ggplot(df, aes(loan_status, fill = loan_status)) +
  geom_bar() +
  theme(legend.position = 'none')
table(df$loan_status)
round(prop.table(table(df$loan_status)), 3)

```

## Preprocessing

### EDA

-   Deal with missing values and outliers

```{r}
colSums(is.na(df))
df["person_emp_length"][is.na(df["person_emp_length"])] <- 0
```

```{r}
df <- df %>%
  mutate(
         loan_intent = as.factor(loan_intent),
         person_home_ownership = as.factor(person_home_ownership),
         cb_person_default_on_file = as.factor(cb_person_default_on_file),
         loan_grade = as.factor(loan_grade))

#loan_status = as.factor(loan_status),
```

-   drop NAs

```{r}
df.rm <- na.omit(df)
sum(is.na(df.rm))
```

### Stadardizing data

-   Label Encoding

```{r}
df.rm$loan_intent <- as.integer(df.rm$loan_intent)
df.rm$person_home_ownership <- as.integer((df.rm$person_home_ownership))
df.rm$cb_person_default_on_file <- as.integer(df.rm$cb_person_default_on_file)
df.rm$loan_grade <- as.integer(df.rm$loan_grade)

df.rm


```

1.1. Normalize data in R - Log Transformation

```{r}
log_scale.df <- log(data.frame(df.rm))
```

1.2. Normalize Data with Min-Max Scaling

```{r}
process <- preProcess(as.data.frame(df.rm), method=c("range"))

norm_scale <- predict(process, as.data.frame(df.rm))
```

1.3. Normalize Data with Standard Scaling

```{r}
scale_data <- as.data.frame(scale(df.rm))
```

1.4 Using Information Value and Weight of Evidence

```{r}
IV <- Information::create_infotables(data = df.rm, y = "loan_status", parallel = FALSE)
print(IV$Summary)
```

If the IV statistic is:

-   Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads)

-   0.02 to 0.1, then the predictor has only a weak relationship to the Goods/Bads odds ratio

-   0.1 to 0.3, then the predictor has a medium strength relationship to the Goods/Bads odds ratio

-   0.3 to 0.5, then the predictor has a strong relationship to the Goods/Bads odds ratio

-   0.5, suspicious relationship (Check once).

So the variables that greater than 0.02 are the one we needed

```{r}
# select vars of IV < 0.02
vars_removed <- IV$Summary %>% as.data.frame %>% 
                                    subset(IV < 0.02) %>% pull(1)
vars_removed
```

## Split the data set

Due to the lack of time, we can only perform on the origin and WoE processed data for this paper.

The first step is to create our practice data set and our test data set. THe practice set is used for training the model. The test set is used to evaluate model accuracy.

We are going to create a number of models so it is necessary to give them numerical designations (1, 2, 3, etc.). We can break the data sets into any sizes we like, even 50-50, but here we use a one-third, two-thirds split.

```{r}
ind <- sample(2, nrow(df.rm), replace = TRUE, prob = c(0.8, 0.2))
df_train <- df.rm [ind == 1, ]
df_test<- df.rm [ind == 2, ]

prop.table(table(df_train$loan_status))

X <- df_train[,!(names(df) %in% "loan_status")]
y <- df_train[["loan_status"]]
```

## Cross Validation

### UpSampling Data

```{r}
#upsampling
df_train_up <- upSample(x = X,
                        y = as.factor(y),
                        yname = "loan_status")

prop.table(table(df_train_up$loan_status))

```

### DownSampling Data

```{r}
df_train_down <-  downSample(x = X,
                             y = as.factor(y),
                             yname = "loan_status")

prop.table(table(df_train_down$loan_status))
```

## IV and WOE

### Performing Binning of features using WoE Analysis

```{r}
train.data_removed<- df_train %>% dplyr::select(-all_of(vars_removed))
```

Weight of Evidence Bining

```{r}
bin = woebin(train.data_removed, y = 'loan_status')
```

### Examplatory Plot

```{r}
woebin_plot(bin)
```

Apply bins We can take the list with all the binning information and pass ist to scorecard::woebin_ply in order to transform our dataset into an all WOE value dataset

```{r}
train.data_woe <- woebin_ply(train.data_removed, bin)
# head(train.data_woe)
```

## Check for correlation of the explainatory variables in the dataset

```{r}
# Correlation Matrix
numericVarName <- names(which(sapply(df.rm, is.numeric)))
corr <- cor(df.rm[, numericVarName], use = "pairwise.complete.obs")
ggcorrplot(corr, lab = FALSE)
```

We can see that the variable that has correlation to most of the variable is Person_age, rather than the loan_status; therefore, we have to modify the data for out model.

# IV. Apply models

## Method One: Logistic Regression

### Perform the logit model for the training set of the initial data

At this stage, we will perform a logistic regression using the glm() function. We start with the the training set. Here, we will be selective with the variables we use in the model. We'll change this in a bit but for now just use five to determine the value of Creditability. First we perform it with the orginal training data:

-   Model fitting

```{r}

logit.model <- glm(df_train$loan_status ~ ., family = binomial(link = 'logit'), data = df_train)
summary(logit.model)
```

Choose variables by stepwise

```{r}
train.step <- step(logit.model, direction = "backward", trace = 0)
summary(train.step)
```

-   Model Validation

```{r}
train.prob <- predict(train.step, type = "response")
train.pred <- ifelse(train.prob > .5, "1", "0")
table(train.pred, df_train$loan_status)
```

-   Model Evaluation

```{r}
logit.pred.prob<- predict(train.step, df_test, type = 'response')
logit.pred <- as.factor(ifelse(logit.pred.prob > 0.5, 1, 0))
df_test$loan_status <- as.factor(df_test$loan_status)
### validation
cf <- caret::confusionMatrix(logit.pred, df_test$loan_status, positive = "1")


# Visualizing Confusion Matrix
fourfoldplot(as.table(cf),color=c("green","red"),main = "Confusion Matrix")
```

We have fitted our model. Now we will use the ROCR package to create predictions and measure performance in terms of Area Under the Curve (AUC). The greater the AUC measure, the better our model is performing.

```{r}
library(ROCR)
pred <- prediction(logit.pred.prob, df_test$loan_status)
perf <- performance(pred, 'tpr', 'fpr')

plot(perf, main = "ROC curve -- Logistic Regression")
```

And we'll wrap this part up by finding the AUC.

```{r}
AUCLog <- performance(pred, measure = 'auc')@y.values[[1]]
AUCLog
```

That's not a bad result, but let's see if we can do better with a different method.

### Perform the logit model for the training set that binned according to WOE

We are going to repeat the same steps as above for the WOE

-   Model fitting

```{r}
logit.model_woe <- glm( train.data_woe$loan_status~., family = binomial(link = 'logit'), data = train.data_woe)
summary(logit.model_woe)
```

```{r}
logit.model.step_woe <- step(logit.model_woe, direction = "backward", trace = 0)
summary(logit.model.step_woe)
```

-   Model Validation

```{r}
train.prob <- predict(logit.model.step_woe, type = "response")
train.pred <- ifelse(train.prob > .5, "1", "0")
table(train.pred, train.data_removed$loan_status)
```

-   Model Evualation

```{r}
test.data_woe <- woebin_ply(df_test, bin)
```

Performing model on test dataset:

```{r}
logit.pred.prob_woe <- predict(logit.model.step_woe, test.data_woe, type = 'response')
logit.pred_woe <- as.factor(ifelse(logit.pred.prob_woe > 0.5, 1, 0))
test.data_woe$loan_status <- as.factor(test.data_woe$loan_status)
### validation
cf <- caret::confusionMatrix(logit.pred_woe, test.data_woe$loan_status, positive = "1")

fourfoldplot(as.table(cf),color=c("green","red"),main = "Confusion Matrix")
```

ROC curve.

```{r}
pred <- prediction(logit.pred.prob_woe, df_test$loan_status)
perf <- performance(pred, 'tpr', 'fpr')

plot(perf, main = "ROC curve -- Logistic Regression")
```

Finding the AUC.

```{r}
AUCLog <- performance(pred, measure = 'auc')@y.values[[1]]
AUCLog
```

It is clearly to see that with the same model but WOE binning variables perform better than the initial data; but until then, let's see if we can do better with a different method.

## Method Two: Regression Tree

Next, let's try analyzing the data using a regression tree approach. Much of our code is similar to what was used in the logistic models above but we need to do some tweaking, which you will recognize.

### Upsampling dataset

```{r}
library(rpart)
TreeModel <- rpart(df_train_up$loan_status ~ ., data = df_train_up)
library(rpart.plot)
prp(TreeModel, type = 2, extra = 1)
```

```{r}
fitTree <- predict(TreeModel, newdata = df_test, type = 'prob')[, 2]
pred <- prediction(fitTree, df_test$loan_status)
```

```{r}
fitTree.pred <- as.factor(ifelse(fitTree > 0.5, 1, 0))
cf <- caret::confusionMatrix(fitTree.pred, as.factor(df_test$loan_status), positive = "1")

fourfoldplot(as.table(cf),color=c("green","red"),main = "Confusion Matrix")
```

```{r}
perf <- performance(pred, 'tpr', 'fpr')
plot(perf)
```

```{r}
AUCTree <- performance(pred, measure = 'auc')@y.values[[1]]
AUCTree
```

These aren't satisfactory results, given all the complexity of our tree model, so again we have to wonder if we aren't better off using the simpler Logistic Regression model from the first example.

## Method Three: Random Forest

Instead of building one decision tree, we can use the random forest method to create a metaphorical "forest" of decision trees. In this method, the end result is the mode of the classes (if we are working on a classification model) or the mean of the predictions (if we are working with regressions).

The idea behind the random forest is that decision trees are prone to overfitting, so finding the "average" tree in the forest can help avoid this problem.

First, perform the model with the upsampling training data set

```{r}
library(randomForest)
RF <- randomForest(df_train_up$loan_status ~. , data = df_train_up)
fitForest1 <- predict(RF, newdata = df_test, type = 'prob')[, 2]
```

```{r}
fitForest.pred <- as.factor(ifelse(fitForest1 > 0.5, 1, 0))
cf <- caret::confusionMatrix(fitForest.pred, as.factor(df_test$loan_status), positive = "1")

fourfoldplot(as.table(cf),color=c("green","red"),main = "Confusion Matrix")
```

```{r}
pred4 <- prediction(fitForest1, df_test$loan_status)
perf4 <- performance(pred4, 'tpr', 'fpr')
plot(perf4) 
```

```{r}
AUCRF <- performance(pred4, measure = 'auc')@y.values[[1]]
AUCRF
```

With the extra effort, we get a somewhat improved result. The random forest model is the best performing of the three we have tried. But it's a judgement call if the results warrant the extra work.

# Show result

```{r}
# Calculate scorecard scores
z<-log(logit.pred.prob_woe/(1-logit.pred.prob_woe))
head(z,10)
```
Calculate scorecard scores for variables based on the results from woebin and glm: 
```{r}
my_card <- scorecard(bin, train.step, points0 = 600, odds0 = 1/19, pdo = 50)
head(my_card)
```

Scorecards based on our model provide scores. A score is a measure that allows lenders to rank customers from high risk (low score) to low risk (high score) and as such provides a relative measure of credit risk. Scores are unlimited and can be measured within any range; they can even be negative. A score is not the same as a probability. A probability also allows us to rank, but on top of that, since it is limited between 0 and 1, it also gives an absolute interpretation of credit risk. Hence, probabilities provide more information than scores do. For application scoring, one does not need well-calibrated probabilities of default. However, for other application areas such as regulatory capital calculation in a Basel setting, as we will discuss later, calibrated default probabilities are needed (Van Gestel and Baesens 2009).


# Reference

References Martens, D., B. Baesens, T. Van Gestel, and J. Vanthienen. 2007. "Comprehensible Credit Scoring Models Using Rule Extraction from Support Vector Machines." European Journal of Operational Research 183:1466--1476.

Baesens, B., Roesch, D., & Scheule, H. (2016). Credit risk analytics: Measurement techniques, applications, and examples in SAS. John Wiley & Sons.

Siddiqi, N. (2012). Credit risk scorecards: developing and implementing intelligent credit scoring. John Wiley & Sons.

Anderson R (2007): The Credit Scoring Toolkit: Theory and Practice for Retail Credit Risk Management and Decision Automation. Oxford, Oxford University Press.

Hand DJ, Henley WE (1997): Statistical Classification Methods in Consumer Credit Scoring: a review. Journal. of the Royal Statistical Society, Series A, 160(3):523--541.

Thomas LC (2000): A survey of credit and behavioural scoring: forecasting financial risk of lending to consumers. International Journal of Forecasting, 16(2):149--172 .

Thomas LC (2009): Consumer Credit Models: Pricing, Profit, and Portfolio. Oxford, Oxford University Press.

Crook JN, Edelman DB, Thomas LC (2007): Recent developments in consumer credit risk assessment. European Journal of Operational Research, 183(3):1447--1465.

Van Gestel, T., B. Baesens, P. Van Dijcke, J. Suykens, J. Garcia, and T. Alderweireld. 2005. "Linear and Nonlinear Credit Scoring by Combining Logistic Regression and Support Vector Machines." Journal of Credit Risk 1, no. 4.

Ben-David, A., & Frank, E. (2009). Accuracy of machine learning models versus "hand crafted" expert systems--a credit scoring case study. Expert Systems with Applications, 36(3), 5264-5271.
